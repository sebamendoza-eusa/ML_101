# Tema 5. Aprendizaje profundo y redes neuronales

## Consideraciones pr√°cticas en tareas de Deep Learning con Keras y TensorFlow

### Flujo de trabajo en un problema de Deep Learning

El desarrollo de un modelo de Deep Learning sigue una serie de pasos esenciales que se pueden resumir a continuaci√≥n

#### Definici√≥n del problema

Antes de dise√±ar una arquitectura de red neuronal, es esencial definir con precisi√≥n el problema a resolver. Algunas preguntas clave incluyen:

- ¬øEs un problema de clasificaci√≥n, regresi√≥n o detecci√≥n de patrones?
- ¬øSe trabaja con datos estructurados, im√°genes o series temporales?
- ¬øCu√°l es el objetivo del modelo? (ejemplo: predecir ventas, clasificar im√°genes, detectar anomal√≠as).
- ¬øCu√°les son las restricciones de datos, hardware y tiempo?

| Tipo de problema  | Ejemplo                                        | Salida esperada              |
| ----------------- | ---------------------------------------------- | ---------------------------- |
| Clasificaci√≥n     | Diagn√≥stico de enfermedades en im√°genes        | Etiqueta discreta (0,1,2...) |
| Regresi√≥n         | Predicci√≥n del precio de una vivienda          | Valor continuo               |
| Series temporales | Predicci√≥n de temperatura en los pr√≥ximos d√≠as | Secuencia de valores         |
| Segmentaci√≥n      | Detecci√≥n de objetos en im√°genes m√©dicas       | Mapa de etiquetas por p√≠xel  |

#### Preprocesamiento de datos

El preprocesamiento es un paso clave para asegurar que los datos sean compatibles con la red neuronal y evitar sesgos o problemas de convergencia.

##### Datos tabulares

- Normalizaci√≥n o estandarizaci√≥n de valores entre `[0,1]` o con media 0 y varianza 1.
- Manejo de valores faltantes mediante imputaci√≥n con la media, mediana o valores especiales.
- Codificaci√≥n de variables categ√≥ricas con one-hot encoding o embeddings.

##### Im√°genes

- Redimensionado para ajustar todas las im√°genes a una misma dimensi√≥n, por ejemplo, `224x224`.
- Normalizaci√≥n de p√≠xeles en el rango `[0,1]`.
- Data augmentation con rotaciones, flips y cambios de brillo para mejorar la generalizaci√≥n.

##### Series temporales

- Aplicaci√≥n de ventana deslizante para transformar datos en secuencias de entrada y salida.
- Diferenciaci√≥n y suavizado para eliminar tendencias.
- Padding y alineaci√≥n para asegurar que todas las secuencias tengan la misma longitud.

> [!tip]
>
> El uso de `tf.data.Dataset` mejora la eficiencia en la carga de datos.

#### Elecci√≥n de arquitectura seg√∫n el problema

Seleccionar la arquitectura adecuada depende del tipo de datos y de la tarea a resolver.

| Tipo de datos         | Arquitectura recomendada | Ejemplo de aplicaci√≥n                     |
| --------------------- | ------------------------ | ----------------------------------------- |
| Datos tabulares       | MLP (red neuronal densa) | Predicci√≥n de ventas, precios             |
| Im√°genes              | CNN (red convolucional)  | Clasificaci√≥n de objetos, visi√≥n          |
| Series temporales     | RNN, LSTM, GRU           | Predicci√≥n financiera, IoT                |
| Detecci√≥n de patrones | Autoencoders, GANs       | An√°lisis de fraudes, s√≠ntesis de im√°genes |

Elegir una arquitectura m√°s compleja solo si es necesario, ya que los modelos m√°s simples tienden a ser m√°s interpretables y eficientes.

#### Entrenamiento y optimizaci√≥n

Una vez que los datos est√°n listos y la arquitectura definida, el siguiente paso es entrenar el modelo.

##### Configuraciones a tener en cuenta

- Tama√±o del batch entre `32` y `128`, ya que valores grandes aceleran el entrenamiento pero pueden afectar la generalizaci√≥n.
- Uso de optimizaci√≥n con `Adam`, aunque `SGD` con ajuste de tasa de aprendizaje puede ser m√°s adecuado en algunos casos.
- Regularizaci√≥n mediante `Dropout`, `L2` y `Batch Normalization` para evitar sobreajuste.
- Ajuste de hiperpar√°metros mediante `GridSearch` o `Optuna`.
- Aplicar reducci√≥n de tasa de aprendizaje con `ReduceLROnPlateau` si la p√©rdida deja de mejorar.

#### Evaluaci√≥n y m√©tricas

El rendimiento de un modelo debe evaluarse con m√©tricas adecuadas al tipo de problema.

| Tipo de problema  | M√©trica principal      | Alternativa                 |
| ----------------- | ---------------------- | --------------------------- |
| Clasificaci√≥n     | Accuracy, F1-score     | AUC, Precision-Recall Curve |
| Regresi√≥n         | MSE, MAE               | R¬≤, RMSE                    |
| Series temporales | RMSE, MAE              | Correlaci√≥n de Pearson      |
| Generaci√≥n        | Perplexity, BLEU score | Rouge-L, Cosine Similarity  |

Si los datos est√°n desbalanceados, evitar accuracy y usar m√©tricas como F1-score o AUC-ROC.

#### Despliegue y consideraciones en producci√≥n

Una vez entrenado, el modelo debe ser eficiente y escalable en producci√≥n.

##### Conversi√≥n y optimizaci√≥n

- Reducci√≥n del tama√±o del modelo mediante `pruning` y `quantization` con `TensorFlow Lite`.
- Exportaci√≥n del modelo en formato `SavedModel` o `ONNX`.
- Inferencia eficiente con `batching` y `caching`.

##### Implementaci√≥n en producci√≥n

- Despliegue mediante API REST con `FastAPI` o `Flask`.
- Uso de `TensorFlow Serving` o `Triton Inference Server` para inferencia en tiempo real.
- Escalabilidad en la nube con AWS, GCP o Azure.

Evaluar el impacto del modelo antes del despliegue con t√©cnicas de explicabilidad como `SHAP` o `LIME`.

### Preprocesamiento de datos

Dependiendo del tipo de datos, los procedimientos de preprocesamiento pueden variar significativamente.

#### Datos estructurados

##### Normalizaci√≥n y estandarizaci√≥n

Las redes neuronales son sensibles a la escala de los datos, por lo que es recomendable aplicar t√©cnicas de normalizaci√≥n o estandarizaci√≥n.

Un ejemplo sencillo de c√≥digo

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler

scaler = MinMaxScaler()  # O usar StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

Ambas t√©cnicas tienen el objetivo de mejorar la estabilidad y convergencia del entrenamiento, evitando gradientes inestables y problemas de escala en los datos. Sin embargo, en Deep Learning influyen factores adicionales, como la activaci√≥n utilizada, la arquitectura y la sensibilidad a la escala de los datos que hay que tener en cuenta para escoger un m√©todo u otro.

###### **Cu√°ndo usar normalizaci√≥n en Deep Learning**

- **Redes con activaci√≥n sigmoide o tanh**
  - Las funciones **sigmoide** y **tanh** son sensibles a la escala de los datos.
  - La normalizaci√≥n ayuda a evitar saturaci√≥n en los extremos de la funci√≥n (donde la derivada es casi cero).
- **Modelos de redes profundas con m√∫ltiples capas ocultas**
  - Facilita la propagaci√≥n del gradiente evitando valores grandes.
  - Es recomendable en arquitecturas **muy profundas** que no utilicen batch normalization.
- **Datos que tienen un rango bien definido**
  - Sensorizaci√≥n o mediciones f√≠sicas (temperatura, presi√≥n, humedad).
  - Datos de p√≠xeles en im√°genes, donde los valores est√°n en `[0,255]` y se normalizan a `[0,1]`.
- **Cuando las unidades de los datos tienen significados distintos**
  - Si hay m√∫ltiples caracter√≠sticas con escalas diferentes (por ejemplo, precio en d√≥lares y peso en kilogramos).
  - Evita que una variable domine el entrenamiento debido a su magnitud.

###### **Cu√°ndo usar estandarizaci√≥n en Deep Learning**

- **Redes con activaci√≥n ReLU o variantes (LeakyReLU, ELU, GELU)**
  - ReLU no est√° limitada en su salida, por lo que la normalizaci√≥n no es tan efectiva como la estandarizaci√≥n.
  - Modelos con ReLU pueden manejar datos con mayor dispersi√≥n sin afectar la convergencia.
- **Cuando los datos tienen una distribuci√≥n desconocida o con valores extremos**
  - La estandarizaci√≥n reduce la influencia de los valores at√≠picos porque ajusta los datos en funci√≥n de la desviaci√≥n est√°ndar.
  - Es preferible si hay variables con alta varianza.
- **Cuando los modelos usan Batch Normalization**
  - La estandarizaci√≥n favorece la estabilizaci√≥n de los valores de activaci√≥n en cada capa.
  - Al aplicar Batch Normalization, los datos se reescalan din√°micamente en cada mini-lote, por lo que una estandarizaci√≥n previa mejora la estabilidad.

##### Manejo de valores faltantes

Los datos estructurados suelen contener valores faltantes que pueden afectar el rendimiento del modelo.

- Eliminaci√≥n de registros si la cantidad de valores nulos es peque√±a.
- Imputaci√≥n con la media o mediana si los valores siguen una distribuci√≥n normal o sesgada.
- Uso de valores especiales como `-999` si el modelo debe aprender la ausencia de datos.

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")  # O usar "mean"
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)
```

##### Codificaci√≥n de variables categ√≥ricas

Las redes neuronales no pueden procesar directamente datos categ√≥ricos, por lo que es necesario codificarlos num√©ricamente.

- One-hot encoding: √ötil para categor√≠as sin orden (ciudad, color, pa√≠s).
- Label encoding: Se usa en categor√≠as con orden l√≥gico (bajo, medio, alto).

Ejemplo de c√≥digo

```python
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import numpy as np

# Datos categ√≥ricos de ejemplo
X_train = np.array(["rojo", "azul", "verde", "azul", "rojo", "verde"]).reshape(-1, 1)

# OneHotEncoder
encoder_ohe = OneHotEncoder(sparse=False)
X_onehot = encoder_ohe.fit_transform(X_train)

print("One-Hot Encoding:")
print(X_onehot)

# LabelEncoder
encoder_label = LabelEncoder()
X_label = encoder_label.fit_transform(X_train.ravel())

print("\nLabel Encoding:")
print(X_label)

```

#### Datos de im√°genes

Las im√°genes deben ser convertidas en **tensores** antes de ser utilizadas en redes neuronales convolucionales (CNN).

##### Redimensionado y normalizaci√≥n

Las im√°genes pueden tener tama√±os variados, por lo que es necesario redimensionarlas a una dimensi√≥n est√°ndar. Adem√°s, los valores de los p√≠xeles deben ser normalizados.

```python
import tensorflow as tf

def preprocess_image(image, label):
    image = tf.image.resize(image, [224, 224])  # Redimensionar a 224x224
    image = image / 255.0  # Normalizaci√≥n en el rango [0,1]
    return image, label
```

##### Aumento de datos

El aumento de datos puede mejorar la generalizaci√≥n del modelo al aplicar transformaciones aleatorias a las im√°genes de entrenamiento.

- Rotaciones y traslaciones para simular variaciones en la toma de im√°genes.
- Flips horizontales y verticales para aumentar la diversidad de los datos.
- Ajuste de brillo y contraste para mejorar la robustez del modelo ante cambios de iluminaci√≥n.

Ejempo de c√≥digo

```python
import tensorflow as tf

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.1)
])
```

#### Datos en series temporales

El preprocesamiento de datos secuenciales, como mediciones de sensores o registros financieros, requiere la conversi√≥n de datos en ventanas de tiempo.

##### Creaci√≥n de ventanas deslizantes

Los modelos de aprendizaje profundo requieren que las series temporales sean transformadas en secuencias de entrada-salida.

Ejemplo de funci√≥n personalizada que genera un dataset seq2seq a partir de ventana deslizante

```python
import numpy as np

def genera_secuencia(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

# Datos de ejemplo (serie temporal)
serie = [100, 102, 105, 110, 120]

X_train_seq, y_train_seq = genera_secuencia(serie, window_size=2)
```

Tambi√©n existe la posibilidad de usar ventanas deslizantes variables, aunque esta t√©cnica es mucho m√°s habitual en NLP. Sin embargo, tambi√©n aparecen en **series temporales, aprendizaje por refuerzo y sistemas de predicci√≥n secuencial**. Son √∫tiles cuando la dependencia entre eventos puede variar en longitud y se desea permitir que el modelo aprenda de secuencias progresivamente m√°s largas.

```python
import numpy as np

def generar_secuencias_variable(serie):
    X, y = [], []
    for i in range(1, len(serie)):
        X.append(serie[:i])  # Secuencia creciente
        y.append(serie[i])    # Predicci√≥n del siguiente valor
    return X, y

# Datos de ejemplo (serie temporal)
serie = [100, 102, 105, 110, 120]

# Generar secuencias variables
X_variable, y_variable = generar_secuencias_variable(serie)
```

##### Diferenciaci√≥n y suavizado

En el **preprocesamiento de series temporales**, la **diferenciaci√≥n** y el **suavizado** son t√©cnicas clave para mejorar la capacidad predictiva del modelo y reducir el ruido en los datos.

La **diferenciaci√≥n** consiste en calcular la diferencia entre valores consecutivos de la serie temporal para eliminar tendencias y hacer que los datos sean estacionarios. Ser√° interesante usar diferenciaci√≥n en los siguientes casos:

- Cuando los datos muestran una **tendencia creciente o decreciente**.
- Para hacer la serie **estacionaria**, lo que es √∫til en modelos como ARIMA o redes LSTM.
- Si los valores absolutos tienen mucha variabilidad, pero los cambios relativos son m√°s predecibles.

Este es un ejemplo pr√°ctico de c√≥digo

```python
import numpy as np
import matplotlib.pyplot as plt

# Serie de datos con tendencia
serie = np.array([100, 105, 110, 120, 130, 145])

# Aplicar diferenciaci√≥n (resta del valor anterior)
serie_diferenciada = np.diff(serie)

print("Serie original:", serie)
print("Serie diferenciada:", serie_diferenciada)

# Visualizaci√≥n
plt.figure(figsize=(8,4))
plt.plot(serie, label="Serie original", marker='o')
plt.plot(range(1, len(serie)), serie_diferenciada, label="Serie diferenciada", marker='s')
plt.legend()
plt.show()
```

Puede verse como la serie diferenciada muestra los incrementos entre valores en lugar de los valores absolutos. Pueden usarse variantes de diferenciaci√≥n **de primer o segundo orden**

Por su parte, el **suavizado** ayuda a reducir el ruido en una serie temporal manteniendo su tendencia principal. Es √∫til cuando los datos contienen **fluctuaciones irregulares** que podr√≠an afectar el modelo. Es decir, los casos en los que usar suavizado ser√≠an:

- Cuando los datos tienen **variabilidad alta o ruido**, lo que dificulta la detecci√≥n de patrones.
- Si el modelo tiene dificultades para aprender debido a **datos demasiado irregulares**.
- Para preprocesar series en **modelos de predicci√≥n financiera, meteorol√≥gica o industrial**.

Existen varias t√©cnicas de suavizado, entre las que destacamos las siguientes

1. **Media m√≥vil simple**: Calcula el promedio de los √∫ltimos *n* valores para cada punto.
2. **Media m√≥vil exponencial (EMA)**: Da m√°s peso a valores recientes.
3. **Filtro de Savitzky-Golay**: Mantiene la estructura de la se√±al mientras la suaviza.

Sin embargo, es muy habitual usar el primero de ellos. A continuaci√≥n tienes un ejemplo de c√≥digo

```python
def media_movil(serie, ventana=3):
    return np.convolve(serie, np.ones(ventana)/ventana, mode='valid')

# Aplicar suavizado con ventana de 3
serie_suavizada = media_movil(serie, ventana=3)

print("Serie original:", serie)
print("Serie suavizada:", serie_suavizada)

# Visualizaci√≥n
plt.figure(figsize=(8,4))
plt.plot(serie, label="Serie original", marker='o')
plt.plot(range(2, len(serie)), serie_suavizada, label="Serie suavizada (Media M√≥vil)", marker='s')
plt.legend()
plt.show()
```

Puede observarse en la salida como la serie suavizada sigue la tendencia general, pero elimina fluctuaciones bruscas.

###### **Comparaci√≥n r√°pida: Diferenciaci√≥n vs. Suavizado**

| **T√©cnica**        | **Objetivo**                                      | **Cu√°ndo usarla**                                            |
| ------------------ | ------------------------------------------------- | ------------------------------------------------------------ |
| **Diferenciaci√≥n** | Eliminar tendencia y hacer la serie estacionaria. | Si los datos tienen tendencia fuerte y el modelo requiere estacionariedad. |
| **Suavizado**      | Reducir ruido y preservar la tendencia.           | Si los datos tienen fluctuaciones aleatorias que dificultan el aprendizaje. |

Es importante tener en cuenta que ambas t√©cnicas pueden combinarse. Por ejemplo, en **predicci√≥n financiera**, es com√∫n **suavizar primero** para reducir ruido y luego **diferenciar** para eliminar tendencia antes de aplicar modelos como LSTM.

A continuaci√≥n tienes un ejemplo de c√≥digo donde se combinan ambas t√©cnicas. Primero se suaviza la serie para despu√©s aplicar una diferenciaci√≥n de primer orden y una posterior normalizaci√≥n de los datos

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# 1. Datos de ejemplo (Serie con tendencia y ruido)
np.random.seed(42)
tiempo = np.arange(30)
serie = 50 + tiempo * 2 + np.random.normal(scale=5, size=len(tiempo))  # Tendencia + Ruido

# 2. Suavizado: Media M√≥vil Simple
def media_movil(serie, ventana=3):
    return np.convolve(serie, np.ones(ventana)/ventana, mode='valid')

serie_suavizada = media_movil(serie, ventana=3)

# 3. Diferenciaci√≥n: Eliminar tendencia
serie_diferenciada = np.diff(serie_suavizada)

# 4. Normalizaci√≥n de los datos (Min-Max Scaling)
scaler = MinMaxScaler(feature_range=(0, 1))
serie_normalizada = scaler.fit_transform(serie_diferenciada.reshape(-1, 1)).flatten()

# Visualizaci√≥n de las transformaciones
plt.figure(figsize=(12,6))

plt.subplot(3,1,1)
plt.plot(serie, label="Serie original", marker='o')
plt.legend()

plt.subplot(3,1,2)
plt.plot(serie_suavizada, label="Serie suavizada (Media M√≥vil)", marker='s')
plt.legend()

plt.subplot(3,1,3)
plt.plot(serie_normalizada, label="Serie diferenciada y normalizada", marker='x')
plt.legend()

plt.show()

```

#### Generaci√≥n de datasets en tf.data

El uso de `tf.data` optimiza el manejo de grandes vol√∫menes de datos y permite aplicar transformaciones en paralelo.

##### Creaci√≥n de dataset desde tensores

```python
import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)
```



### **Modelizaci√≥n y arquitecturas en Deep Learning**

La selecci√≥n de la arquitectura en Deep Learning depende del tipo de problema y los datos disponibles. A continuaci√≥n, se describen los principales enfoques y consideraciones pr√°cticas para cada tipo de red.

#### **Perceptr√≥n y MLP**

Las redes neuronales multicapa (MLP) son la opci√≥n cl√°sica para trabajar con **datos tabulares**, problemas de **regresi√≥n** y **clasificaci√≥n**. Se basan en capas completamente conectadas (*fully connected layers*), lo que permite modelar relaciones entre variables de entrada.

##### **Casos de uso**

- **Regresi√≥n num√©rica:** Predicci√≥n de valores continuos (ejemplo: precios de viviendas).
- **Clasificaci√≥n:** Identificaci√≥n de categor√≠as a partir de datos estructurados.
- **Procesamiento de datos tabulares:** Problemas en los que los atributos no tienen relaciones espaciales o temporales claras.

##### **Selecci√≥n de capas y funciones de activaci√≥n**

- **N√∫mero de capas ocultas:** Entre `1 y 3` suele ser suficiente para datos tabulares.
- **N√∫mero de neuronas:** Se suele elegir entre el tama√±o de la entrada y el tama√±o de la salida.
- **Activaciones recomendadas:**
  - `ReLU`: Funci√≥n est√°ndar en capas ocultas para evitar el problema del gradiente.
  - `Softmax`: Para clasificaci√≥n multiclase en la capa de salida.
  - `Sigmoid`: Para clasificaci√≥n binaria.

Ejemplo de un modelo que se puede aplicar a tareas de **clasificaci√≥n binaria**.

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Suponemos que se han generado un X_train y un y_train

# Definir el modelo de perceptr√≥n
modelo = Sequential([
    Dense(1, activation='sigmoid', input_dim=X_train.shape[1])  # Uso de input_dim en lugar de input_shape
])

# Compilaci√≥n del modelo
modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Resumen del modelo
modelo.summary()

```

La siguiente tabla resume las opciones de trabajo habituales con un perceptron

| **Tarea**                    | **Salida**              | **Activaci√≥n** | **Funci√≥n de p√©rdida**     |
| ---------------------------- | ----------------------- | -------------- | -------------------------- |
| **Clasificaci√≥n binaria**    | 1 neurona (`Dense(1)`)  | `sigmoid`      | `binary_crossentropy`      |
| **Clasificaci√≥n multiclase** | N neuronas (`Dense(N)`) | `softmax`      | `categorical_crossentropy` |
| **Regresi√≥n**                | 1 neurona (`Dense(1)`)  | `linear`       | `mean_squared_error (MSE)` |

##### Cu√°ndo usar perceptrones y cu√°ndo usar redes MLP

El **perceptr√≥n simple** es √∫til para problemas **linealmente separables**, pero en la mayor√≠a de los casos del mundo real, los datos no son perfectamente lineales. Para abordar problemas m√°s complejos, es necesario **introducir capas ocultas**, convirtiendo el modelo en una **red MLP**.

A continuaci√≥n, se presentan algunos escenarios en los que un **perceptr√≥n simple no es adecuado**, y se recomienda pasar a una **MLP**.

- **Clasificaci√≥n de datos no linealmente separables**. Por ejemplo:
  - Identificar la regi√≥n donde caen puntos en un plano cartesiano. Una red MLP puede aprender **fronteras de decisi√≥n m√°s complejas** usando combinaciones no lineales de caracter√≠sticas.
  - Clasificaci√≥n de im√°genes simples donde las diferencias entre clases no pueden ser expresadas con una √∫nica frontera lineal.
  - Problemas como **XOR**, donde los datos positivos y negativos no pueden separarse con una sola l√≠nea.

- **Procesamiento de datos estructurados con interacciones complejas**. Por ejemplo:
  - Un modelo que predice si un cliente comprar√° un producto en funci√≥n de m√∫ltiples factores (edad, ingresos, frecuencia de compra). ‚Üí la red MLP aprende **interacciones no evidentes entre variables** y puede capturar patrones m√°s complejos.
  - Predicci√≥n de fraudes bancarios donde m√∫ltiples variables interact√∫an de forma no trivial.
  - Diagn√≥stico m√©dico con datos tabulares en los que varias caracter√≠sticas pueden combinarse para indicar una condici√≥n.

- **Reducci√≥n de dimensionalidad y extracci√≥n de caracter√≠sticas**. Por ejemplo:
  - Un conjunto de datos con **m√∫ltiples atributos redundantes** ‚Üí En **an√°lisis de clientes**, variables como ‚Äúsalario‚Äù y ‚Äúgasto mensual‚Äù pueden estar correlacionadas. Una **MLP** puede aprender representaciones m√°s compactas sin necesidad de ingenier√≠a de caracter√≠sticas manual.

- **Tareas que requieren abstracciones jer√°rquicas**. Por ejemplo:
  - Clasificaci√≥n de im√°genes donde es necesario detectar bordes, texturas y formas antes de clasificar un objeto ‚Üí Una red MLP puede ser √∫til en el reconocimiento de escritura manual o clasificaci√≥n de im√°genes simples en datasets peque√±os donde no se requiere una CNN.

Pasar de un modelo de perceptr√≥n a una red MLP tiene riesgos y ventajas. Las ventajas podr√≠an ser las siguientes:

- **Captura de relaciones no lineales**: Una MLP con activaciones no lineales (como `ReLU` o `sigmoid`) puede modelar datos m√°s complejos.
- **Mejor generalizaci√≥n**: Al aumentar la capacidad del modelo, se pueden aprender patrones m√°s robustos.
- **Posibilidad de ajuste fino**: Permite optimizar la arquitectura agregando m√°s neuronas o capas seg√∫n sea necesario.
- **Extracci√≥n de caracter√≠sticas**: Las capas ocultas pueden aprender representaciones √∫tiles del problema sin necesidad de un dise√±o manual de atributos.

Pero tambi√©n existen riesgos:

- **Mayor riesgo de sobreajuste**: Una red m√°s grande puede memorizar en lugar de generalizar, especialmente en conjuntos de datos peque√±os.
- **Mayor costo computacional**: M√°s neuronas y capas requieren mayor poder de c√≥mputo y tiempos de entrenamiento m√°s largos.
- **M√°s hiperpar√°metros a ajustar**: N√∫mero de capas, neuronas, tasa de aprendizaje, regularizaci√≥n‚Ä¶ aumenta la complejidad del ajuste del modelo.
- **Dificultad en la interpretabilidad**: Un perceptr√≥n es f√°cil de interpretar, pero una MLP se vuelve una ‚Äúcaja negra‚Äù.

###### Conclusiones ¬øCu√°ndo NO ES recomendable pasar a una MLP?

Cuando ocurra alguna de las siguientes situaciones:

- **Los datos son linealmente separables** (ejemplo: un dataset simple con dos clases claramente diferenciadas).
- **El dataset es muy peque√±o**, ya que una MLP puede sobreajustarse con facilidad.
- **El modelo debe ser interpretable**, como en aplicaciones donde es crucial entender por qu√© se toma una decisi√≥n (por ejemplo, en finanzas o salud).
- **El problema no justifica la complejidad extra**, como en tareas simples de regresi√≥n o clasificaci√≥n con pocas variables relevantes

#### **Redes convolucionales (CNN)**

Las **CNN** son especialmente efectivas en tareas de **procesamiento de im√°genes**, aunque tambi√©n se aplican en otros dominios del DL. Su estructura captura **patrones espaciales** mediante operaciones de convoluci√≥n.

##### **Preprocesamiento de im√°genes**

A continuaci√≥n vamos a detallar algunas consideraciones pr√°cticas en el caso de preprocesamiento de im√°genes a la hora de usar redes convolucionales ya que es un paso fundamental para optimizar el rendimiento de una **Red Neuronal Convolucional (CNN)**.

Keras proporciona herramientas espec√≠ficas para manejar im√°genes antes de alimentarlas a una red neuronal convolucional

###### **Carga de im√°genes con `ImageDataGenerator`**

`ImageDataGenerator` permite cargar im√°genes desde directorios y aplicar transformaciones en tiempo real. Veamos un ejemplo de c√≥mo funciona.

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rescale=1./255,         # Normalizaci√≥n de valores de p√≠xeles a [0,1]
    rotation_range=20,      # Rotaci√≥n aleatoria de 20 grados
    width_shift_range=0.2,  # Desplazamiento horizontal del 20%
    height_shift_range=0.2, # Desplazamiento vertical del 20%
    shear_range=0.2,        # Transformaci√≥n de corte
    zoom_range=0.2,         # Zoom aleatorio del 20%
    horizontal_flip=True,   # Inversi√≥n horizontal
    fill_mode='nearest'     # Modo de relleno de p√≠xeles fuera del marco
)

train_generator = datagen.flow_from_directory(
    'dataset/train',        # Carpeta donde est√°n las im√°genes
    target_size=(150, 150), # Redimensionar todas las im√°genes a 150x150
    batch_size=32,
    class_mode='categorical' # Modo de clasificaci√≥n (binario/multiclase)
)

# Obtener un batch de datos
X_batch, y_batch = next(train_generator)

# Ver formas de los datos generados
print(f"Forma de X_batch: {X_batch.shape}")  # (32, 150, 150, 3)
print(f"Forma de y_batch: {y_batch.shape}")  # (32, N_clases)
```

`ImageDataGenerator` devuelve **batches de im√°genes y etiquetas en formato NumPy**, listos para ser utilizados en el entrenamiento de un modelo CNN en Keras. Es interesante usar  `ImageDataGenerator` cuando:

- Se tiene un **dataset de im√°genes almacenadas en carpetas**.
- Se quiere realizar **data augmentation** en tiempo real.
- Se tiene un **dataset peque√±o** y se necesita aumentar su diversidad.

###### **Preprocesamiento manual de im√°genes**

Si las im√°genes ya est√°n cargadas en memoria como matrices NumPy, se pueden procesar directamente.

Por ejemplo, para cargar im√°genes desde archivos y transformarlas en tensores podemos seguir el siguiente c√≥digo de ejemplo

```python
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import numpy as np

# Cargar imagen
img_path = 'imagen.jpg'
img = image.load_img(img_path, target_size=(150, 150))  # Redimensionar

# Convertir imagen a tensor NumPy
img_array = image.img_to_array(img)

# Expandir dimensi√≥n para simular batch (CNN espera batch de im√°genes)
img_array = np.expand_dims(img_array, axis=0)

# Normalizaci√≥n (opcional)
img_array /= 255.0

print(f"Forma de la imagen procesada: {img_array.shape}")
```

Podemos usar preprocesamiento manual cuando:

- Se trabaja con **im√°genes individuales** en tareas de inferencia.
- Se tiene **control total sobre la manipulaci√≥n de los datos**.

###### **Conversi√≥n de im√°genes en `tf.data.Dataset`**

Para mejorar el rendimiento del entrenamiento en TensorFlow, se puede utilizar `tf.data.Dataset` para manejar grandes vol√∫menes de im√°genes de manera eficiente.

```python
import tensorflow as tf

# Funci√≥n para cargar y preprocesar im√°genes
def cargar_y_preprocesar_imagen(ruta):
    img = tf.io.read_file(ruta)
    img = tf.image.decode_jpeg(img, channels=3)  # Decodificar JPEG
    img = tf.image.resize(img, [150, 150])  # Redimensionar
    img = img / 255.0  # Normalizaci√≥n
    return img

# Lista de rutas de im√°genes
rutas_imagenes = tf.data.Dataset.list_files("dataset/train/*.jpg")

# Crear dataset aplicando la funci√≥n de preprocesamiento
dataset = rutas_imagenes.map(cargar_y_preprocesar_imagen).batch(32).prefetch(tf.data.AUTOTUNE)
```

###### **T√©cnicas clave de preprocesamiento y su impacto en CNN**

Se resumen en la siguiente tabla:

| **T√©cnica**             | **Descripci√≥n**                                             | **Cu√°ndo usarla**                             |
| ----------------------- | ----------------------------------------------------------- | --------------------------------------------- |
| **Normalizaci√≥n**       | Convierte valores de p√≠xeles a `[0,1]` o `[-1,1]`           | Siempre (mejora la estabilidad)               |
| **Estandarizaci√≥n**     | Sustrae la media y divide por la desviaci√≥n est√°ndar        | Cuando la distribuci√≥n es desigual            |
| **Redimensionamiento**  | Ajusta el tama√±o de la imagen                               | Si el modelo requiere entradas de tama√±o fijo |
| **Data augmentation**   | Genera variaciones artificiales de las im√°genes             | Para evitar sobreajuste en datasets peque√±os  |
| **Conversi√≥n a tensor** | Convierte im√°genes en arrays NumPy o tensores de TensorFlow | Para entrenamiento eficiente                  |

##### **Consideraciones pr√°cticas en la modelizaci√≥n de distintas arquitecturas CNN**

Al modelizar redes convolucionales (CNN), la arquitectura debe adaptarse al **tama√±o del dataset, la complejidad del problema y la capacidad computacional**. A continuaci√≥n, se presentan consideraciones pr√°cticas para **arquitecturas desde simples hasta avanzadas**.

###### **Arquitectura CNN simple (para datasets peque√±os)**

Casos de uso:

- Cuando el dataset es **peque√±o (~<10.000 im√°genes)** y se busca un modelo liviano.  
- Para problemas de **clasificaci√≥n binaria o multiclase** con im√°genes simples.  

Estrategia recomendada:
- Pocas capas convolucionales (2-4).
- Filtros peque√±os (`3x3`).
- Capas de pooling (`MaxPooling2D`) para reducir dimensionalidad.
- **Evitar sobreajuste** con `Dropout` (30-50%).

Ejemplo de modelo CNN simple:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

modelo = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),
    MaxPooling2D(pool_size=(2,2)),
    
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # Reduce sobreajuste
    Dense(10, activation='softmax')  # 10 clases
])

modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```
###### **Resumen de mejores pr√°cticas seg√∫n arquitectura**
| **Arquitectura**       | **Dataset ideal**       | **Caracter√≠sticas clave**                             | **Cu√°ndo usarla**                               |
| ---------------------- | ----------------------- | ----------------------------------------------------- | ----------------------------------------------- |
| **CNN simple**         | Peque√±o (<10k im√°genes) | 2-4 capas, MaxPooling, Dropout                        | Clasificaci√≥n b√°sica                            |
| **VGG16/VGG19**        | Mediano (~50k im√°genes) | Profundidad moderada, Transfer Learning               | Clasificaci√≥n de im√°genes con Transfer Learning |
| **ResNet50/101**       | Grande (>100k im√°genes) | Atajos residuales, evita desvanecimiento de gradiente | Im√°genes con muchos detalles                    |
| **YOLO, Faster R-CNN** | Variable                | Bounding boxes, detecci√≥n de objetos                  | Detecci√≥n de m√∫ltiples objetos en im√°genes      |

###### Caso pr√°ctico completo de procesamiento de im√°genes y clasificaci√≥n binaria

El presente caso cubre **todo el flujo de trabajo** desde la carga de im√°genes hasta el entrenamiento de una **CNN simple** en Keras, siguiendo las mejores pr√°cticas. Supongamos que estamos ante un problema de clasificaci√≥n de im√°genes en dos categor√≠as: **perros** üê∂ y **gatos** üê±. Las im√°genes est√°n organizadas en dos carpetas dentro de un directorio

```
dataset/
‚îú‚îÄ‚îÄ perros/  # 1000 im√°genes de perros
‚îú‚îÄ‚îÄ gatos/   # 1000 im√°genes de gatos
```

Cada imagen tiene dimensiones distintas y est√° en color (`RGB`).

Pasos en el c√≥digo:

1. **Carga de im√°genes y preprocesamiento** con `ImageDataGenerator`.
2. **Definici√≥n de una CNN simple** para la clasificaci√≥n binaria.
3. **Entrenamiento del modelo** usando las im√°genes preprocesadas.
4. **Evaluaci√≥n y predicci√≥n en nuevas im√°genes**.

```python
# Importar librer√≠as necesarias
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# 1Ô∏è‚É£ CONFIGURACI√ìN Y PREPROCESAMIENTO DE IM√ÅGENES

# Directorio donde est√°n las im√°genes organizadas en carpetas (perros/gatos)
ruta_dataset = "dataset/"

# Definir generador de datos con aumentaci√≥n para entrenar la red
datagen_entrenamiento = ImageDataGenerator(
    rescale=1./255,  # Normalizaci√≥n: escala valores entre 0 y 1
    rotation_range=20,  # Rotar im√°genes aleatoriamente hasta 20 grados
    width_shift_range=0.1,  # Peque√±os desplazamientos horizontales
    height_shift_range=0.1,  # Peque√±os desplazamientos verticales
    shear_range=0.2,  # Transformaci√≥n en corte
    zoom_range=0.2,  # Zoom aleatorio hasta 20%
    horizontal_flip=True,  # Voltear im√°genes horizontalmente
    validation_split=0.2  # Separar un 20% de datos para validaci√≥n
)

# Cargar im√°genes para entrenamiento
generador_train = datagen_entrenamiento.flow_from_directory(
    ruta_dataset,  # Carpeta ra√≠z con subcarpetas de clases
    target_size=(150, 150),  # Redimensionar todas las im√°genes a 150x150 p√≠xeles
    batch_size=32,  # Tama√±o de lotes
    class_mode="binary",  # Clasificaci√≥n binaria (perros o gatos)
    subset="training"  # Usar el 80% de los datos para entrenamiento
)

# Cargar im√°genes para validaci√≥n
generador_validacion = datagen_entrenamiento.flow_from_directory(
    ruta_dataset,
    target_size=(150, 150),
    batch_size=32,
    class_mode="binary",
    subset="validation"  # Usar el 20% restante para validaci√≥n
)

# 2Ô∏è‚É£ DEFINICI√ìN DEL MODELO CNN

# Modelo CNN simple
modelo = Sequential([
    # Primera capa convolucional con 32 filtros de 3x3
    Conv2D(32, (3,3), activation="relu", input_shape=(150, 150, 3)),
    MaxPooling2D(pool_size=(2,2)),  # Reduce dimensiones de 150x150 -> 75x75
    
    # Segunda capa convolucional con 64 filtros
    Conv2D(64, (3,3), activation="relu"),
    MaxPooling2D(pool_size=(2,2)),  # Reduce dimensiones a 37x37
    
    # Tercera capa convolucional con 128 filtros
    Conv2D(128, (3,3), activation="relu"),
    MaxPooling2D(pool_size=(2,2)),  # Reduce dimensiones a 18x18
    
    # Aplanado de la imagen en un vector
    Flatten(),
    
    # Capa densa con 128 neuronas
    Dense(128, activation="relu"),
    Dropout(0.5),  # Regularizaci√≥n para evitar sobreajuste
    
    # Capa de salida con activaci√≥n sigmoide para clasificaci√≥n binaria
    Dense(1, activation="sigmoid")
])

# 3Ô∏è‚É£ COMPILACI√ìN DEL MODELO

modelo.compile(
    optimizer=Adam(learning_rate=0.0001),  # Adam con tasa de aprendizaje baja para estabilidad
    loss="binary_crossentropy",  # P√©rdida adecuada para clasificaci√≥n binaria
    metrics=["accuracy"]  # M√©trica de precisi√≥n
)

# 4Ô∏è‚É£ ENTRENAMIENTO DEL MODELO

# Entrenar la CNN usando el generador de im√°genes
historial = modelo.fit(
    generador_train,  # Conjunto de entrenamiento
    validation_data=generador_validacion,  # Conjunto de validaci√≥n
    epochs=10,  # N√∫mero de iteraciones sobre el dataset
    verbose=1  # Mostrar progreso
)

# 5Ô∏è‚É£ EVALUACI√ìN Y PRUEBA EN NUEVAS IM√ÅGENES

import numpy as np
from tensorflow.keras.preprocessing import image

def predecir_imagen(ruta_imagen):
    """ Funci√≥n para predecir si una imagen es de un perro o un gato """
    
    img = image.load_img(ruta_imagen, target_size=(150, 150))  # Cargar imagen
    img_array = image.img_to_array(img)  # Convertir a array NumPy
    img_array = np.expand_dims(img_array, axis=0)  # Expandir dimensiones para modelo
    img_array /= 255.0  # Normalizar
    
    prediccion = modelo.predict(img_array)  # Realizar predicci√≥n
    clase = "Perro" if prediccion[0][0] > 0.5 else "Gato"  # Interpretar resultado
    
    print(f"Predicci√≥n: {clase} (confianza: {prediccion[0][0]:.4f})")

# Ejemplo de predicci√≥n
predecir_imagen("ejemplo.jpg")
```

**Explicaci√≥n de cada paso**

**1Ô∏è‚É£ Preprocesamiento de im√°genes**

- Se usa `ImageDataGenerator` para **cargar, redimensionar y normalizar** im√°genes.
- Se aplican **t√©cnicas de aumentaci√≥n de datos** (rotaciones, zoom, desplazamientos) para mejorar la generalizaci√≥n.
- Se separa **80% para entrenamiento y 20% para validaci√≥n** con `validation_split=0.2`.

**2Ô∏è‚É£ Definici√≥n del modelo CNN**

- Se usa una arquitectura con **3 capas convolucionales** y `MaxPooling2D` para reducir dimensiones.
- La activaci√≥n **ReLU** ayuda a mantener la no linealidad.
- Se usa `Dropout(0.5)` para **evitar sobreajuste** en la capa densa.

**3Ô∏è‚É£ Compilaci√≥n del modelo**

- Se usa **Adam** con `learning_rate=0.0001` para estabilidad.
- Se define la funci√≥n de p√©rdida **`binary_crossentropy`** para clasificaci√≥n binaria.
- Se monitorea la **precisi√≥n (`accuracy`)**.

**4Ô∏è‚É£ Entrenamiento del modelo**

- Se usa `modelo.fit()` con `epochs=10` (ajustable seg√∫n rendimiento).
- Se entrena en im√°genes ya **preprocesadas con `ImageDataGenerator`**.

**5Ô∏è‚É£ Evaluaci√≥n y predicci√≥n**

- Se implementa `predecir_imagen()` para cargar y predecir im√°genes nuevas.
- Se normaliza la imagen y se pasa por la red CNN para obtener **perro o gato**.

#### **Redes recurrentes (RNN, LSTM, GRU)**

Las **redes recurrentes** est√°n dise√±adas para trabajar con **secuencias de datos** y capturar dependencias temporales. Se aplican en tareas como **series temporales, procesamiento de lenguaje natural y predicci√≥n de se√±ales**.

##### **Aplicaciones en series temporales**

- **Predicci√≥n financiera:** Modelado de tendencias en bolsas de valores.
- **An√°lisis de sensores:** Predicci√≥n de fallos en sistemas industriales.
- **Reconocimiento de voz:** Procesamiento de se√±ales de audio.

##### **Diferencias entre RNN, LSTM y GRU**

| **Arquitectura** | **Ventajas**                                            | **Desventajas**                                             |
| ---------------- | ------------------------------------------------------- | ----------------------------------------------------------- |
| **RNN**          | Captura dependencias temporales.                        | Problema de gradientes desvanecientes, dif√≠cil de entrenar. |
| **LSTM**         | Maneja dependencias largas mediante puertas de control. | M√°s costosa computacionalmente.                             |
| **GRU**          | Similar a LSTM pero con menos par√°metros.               | Puede no capturar dependencias tan largas como LSTM.        |

### **Optimizaci√≥n y entrenamiento en Deep Learning**

El entrenamiento de redes neuronales profundas requiere el ajuste cuidadoso de varios elementos clave, como el **algoritmo de optimizaci√≥n, los hiperpar√°metros y las t√©cnicas de regularizaci√≥n**. Veamos algunas consideraciones pr√°cticas para mejorar el rendimiento de los modelos de deep learning.

#### **Optimizaci√≥n en redes neuronales**

Los modelos de deep learning entrenan mediante **descenso de gradiente**, donde el objetivo es minimizar una funci√≥n de p√©rdida ajustando los pesos del modelo. Existen diferentes algoritmos de optimizaci√≥n que afectan la rapidez y estabilidad del entrenamiento.

| **Optimizador**                       | **Descripci√≥n**                                              | **Cu√°ndo usarlo**                                            |
| ------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **SGD (Stochastic Gradient Descent)** | Optimizaci√≥n cl√°sica basada en gradiente descendente. Puede incluir momentum. | Modelos grandes, pero con una tasa de aprendizaje bien ajustada. Puede ser inestable si no se usa momentum. |
| **Adam**                              | Combina **momentum** y **RMSprop**, adaptando la tasa de aprendizaje en cada paso. | Recomendado por defecto en la mayor√≠a de los modelos. Estable y r√°pido en convergencia. |
| **RMSprop**                           | Divide la tasa de aprendizaje entre la media cuadr√°tica de gradientes recientes. | Bueno para **series temporales y RNN**, donde los gradientes pueden volverse inestables. |

En **TensorFlow/Keras**, el optimizador se define en la compilaci√≥n del modelo:

```python
from tensorflow.keras.optimizers import Adam

modelo.compile(optimizer=Adam(learning_rate=0.001), loss="categorical_crossentropy", metrics=["accuracy"])
```

Consideraciones pr√°cticas

- **Si el modelo no converge**: prueba **Adam** con una tasa de aprendizaje m√°s baja.
- **Si el modelo oscila mucho**: usa **SGD con momentum** (`momentum=0.9`).
- **Si hay problemas de gradiente en RNN**: prueba **RMSprop**.

##### **Ajuste de hiperpar√°metros**

El rendimiento del modelo depende de la configuraci√≥n de los **hiperpar√°metros**, como el tama√±o del batch, la tasa de aprendizaje y la arquitectura.

###### **Batch size**

Define cu√°ntas muestras se procesan antes de actualizar los pesos. Se recomienda:

| **Tama√±o del dataset**     | **Batch size recomendado** |
| -------------------------- | -------------------------- |
| Peque√±o (<10,000 muestras) | 8 - 32                     |
| Mediano (10,000 - 100,000) | 32 - 128                   |
| Grande (>100,000)          | 128 - 512                  |

Ejemplo en Keras:

```python
modelo.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))
```

Consideraciones pr√°cticas:

- **Batch peque√±o (<32)**: mejora la generalizaci√≥n, pero el entrenamiento es m√°s lento.
- **Batch grande (>128)**: acelera el entrenamiento, pero puede reducir la capacidad de generalizaci√≥n.

###### **Tasa de aprendizaje (`learning_rate`)**

Controla cu√°nto cambian los pesos en cada iteraci√≥n.

| **Problema**         | **Posible soluci√≥n**     |
| -------------------- | ------------------------ |
| Modelo no aprende    | Aumentar `learning_rate` |
| P√©rdida oscila mucho | Reducir `learning_rate`  |

Ajuste con `Adam`:

```python
Adam(learning_rate=0.0001)
```

##### **Dropout**

Reduce el sobreajuste eliminando neuronas aleatorias en cada iteraci√≥n:

```python
from tensorflow.keras.layers import Dropout

modelo.add(Dense(128, activation="relu"))
modelo.add(Dropout(0.5))  # Desactiva el 50% de las neuronas en cada paso
```

Cu√°ndo usarlo:

- En **capas densas** (`0.2 - 0.5` es un buen rango).
- No se usa en **redes convolucionales antes de Flatten** (puede eliminar demasiada informaci√≥n).

##### **T√©cnicas de regularizaci√≥n**

La regularizaci√≥n evita que el modelo se sobreajuste a los datos de entrenamiento.

###### **L2 Regularization (Weight Decay)**

A√±ade una penalizaci√≥n sobre los pesos grandes:

```python
from tensorflow.keras.regularizers import l2

modelo.add(Dense(64, activation="relu", kernel_regularizer=l2(0.01)))
```

Cu√°ndo usarlo:

- Si el modelo se sobreajusta.
- En **MLP y CNN** para restringir pesos excesivos.

###### **Batch Normalization**

Normaliza la activaci√≥n en cada capa para estabilizar el entrenamiento:

```python
from tensorflow.keras.layers import BatchNormalization

modelo.add(Dense(128, activation="relu"))
modelo.add(BatchNormalization())  # Normaliza despu√©s de la activaci√≥n
```

Cu√°ndo usarlo:

- Si el modelo es **muy profundo**.
- Si la p√©rdida fluct√∫a demasiado durante el entrenamiento.

##### **Resumen de estrategias clave** de optimizaci√≥n

| **Problema detectado** | **Soluci√≥n pr√°ctica**                                       |
| ---------------------- | ----------------------------------------------------------- |
| Modelo no aprende      | Aumentar `learning_rate`, probar `SGD + momentum`           |
| Sobreajuste            | A√±adir `Dropout`, `L2 regularization`, `BatchNormalization` |
| Entrenamiento lento    | Usar `batch_size` m√°s grande, optimizador Adam              |
| P√©rdida no mejora      | Usar `ReduceLROnPlateau`, probar `BatchNormalization`       |
| P√©rdida oscila mucho   | Reducir `learning_rate`, usar `SGD` en lugar de Adam        |

##### **Ejemplo de implementaci√≥n completa de optimizaciones**

A continuaci√≥n tienes un ejemplo de un modelo **entrenado con todas las optimizaciones**:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Definir modelo CNN
modelo = Sequential([
    Conv2D(32, (3,3), activation="relu", input_shape=(150, 150, 3)),
    MaxPooling2D(pool_size=(2,2)),
    BatchNormalization(),
    
    Conv2D(64, (3,3), activation="relu"),
    MaxPooling2D(pool_size=(2,2)),
    
    Conv2D(128, (3,3), activation="relu"),
    MaxPooling2D(pool_size=(2,2)),
    
    Flatten(),
    Dense(128, activation="relu"),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

# Compilar modelo con Adam
modelo.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

# Definir callbacks
early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6)

# Entrenar modelo
modelo.fit(
    X_train, y_train,
    batch_size=32,
    epochs=20,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, reduce_lr]
)
```

#### **Consideraciones pr√°cticas en la fase de an√°lisis del modelo**  

El entrenamiento de un modelo en deep learning no solo depende de los hiperpar√°metros, sino tambi√©n de una correcta inspecci√≥n del modelo y de estrategias pr√°cticas para gestionar su entrenamiento de manera eficiente. 

##### **Uso de `model.summary()` para analizar la arquitectura del modelo**  

Antes de entrenar un modelo, es fundamental verificar su arquitectura, el n√∫mero de par√°metros y la conectividad entre capas. La funci√≥n `model.summary()` en Keras proporciona esta informaci√≥n de manera clara y estructurada.  

Ejemplo de uso para un modelo CNN:

```python
modelo.summary()
```
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 148, 148, 32)      896       
max_pooling2d (MaxPooling2D)  (None, 74, 74, 32)       0         
conv2d_1 (Conv2D)            (None, 72, 72, 64)       18496     
max_pooling2d_1 (MaxPooling2D) (None, 36, 36, 64)     0         
flatten (Flatten)            (None, 82944)            0         
dense (Dense)                (None, 128)              10616960  
dropout (Dropout)            (None, 128)              0         
dense_1 (Dense)              (None, 1)                129       
=================================================================
Total params: 10,635,481
Trainable params: 10,635,481
Non-trainable params: 0
_________________________________________________________________
```

---

Al usar `model.summary()` es importante considerar las siguientes cuestiones: 

- **Verificaci√≥n de la estructura del modelo:**  
   - Comprueba que cada capa tiene las dimensiones esperadas.
   - Verifica que la capa de salida tiene la cantidad correcta de neuronas y activaci√≥n (`softmax` para clasificaci√≥n multiclase, `sigmoid` para clasificaci√≥n binaria, etc.).

- **N√∫mero de par√°metros entrenables:**  
   - Modelos muy grandes pueden necesitar m√°s datos o regularizaci√≥n.
   - Si el n√∫mero de **par√°metros no entrenables** es alto, revisa si hay capas congeladas (por ejemplo, en Transfer Learning).

- **Consistencia de la propagaci√≥n de dimensiones:**  
   - La salida de cada capa debe coincidir con la entrada de la siguiente.
   - En redes convolucionales, el tama√±o de las im√°genes debe ir reduci√©ndose progresivamente antes de llegar a `Flatten()`.

- **Uso en modelos preentrenados:**  
   - Cuando se usa Transfer Learning, es √∫til verificar cu√°ntos par√°metros son **entrenables** (`Trainable params`) y cu√°ntos se mantienen fijos (`Non-trainable params`).

#### **Consideraciones pr√°cticas en la fase de entrenamiento**  

Una vez que el modelo ha sido correctamente definido y compilado, es importante asegurarse de que el entrenamiento se realice de manera eficiente. A continuaci√≥n, se detallan las estrategias pr√°cticas a considerar durante el entrenamiento del modelo.

##### **Monitorizaci√≥n del progreso del entrenamiento**  
En cada √©poca hay que revisar:
- **P√©rdida de entrenamiento (`loss`)**: Debe disminuir progresivamente.  
- **P√©rdida de validaci√≥n (`val_loss`)**: No debe disminuir demasiado r√°pido ni aumentar antes que la p√©rdida de entrenamiento (signo de sobreajuste).  
- **Precisi√≥n (`accuracy`)**: Debe mejorar gradualmente, pero sin saltos abruptos.  
- **Diferencia entre `loss` y `val_loss`**:  
  - Si `val_loss` comienza a subir mientras `loss` sigue bajando ‚Üí posible sobreajuste.  
  - Si `loss` y `val_loss` no bajan ‚Üí posible tasa de aprendizaje incorrecta.  

Un ejemplo de salida esperada durante el entrenamiento podr√≠a ser el siguiente
```
Epoch 5/20
1000/1000 [==============================] - 5s 5ms/step - loss: 0.45 - accuracy: 0.85 - val_loss: 0.50 - val_accuracy: 0.82
```
- Si **`val_loss` es menor que `loss`**, es probable que el modelo est√© generalizando bien.  
- Si **`val_loss` sube mientras `loss` sigue bajando**, considera **Early Stopping** o **Regularizaci√≥n**.  

##### **Estrategias para mejorar el entrenamiento**

###### Para mejorar convergencia y estabilidad  

- **Si la p√©rdida no mejora o es inestable**:  
   - Reducir la **tasa de aprendizaje** (`learning_rate`).
   - Usar **Batch Normalization** para estabilizar la activaci√≥n en cada capa.
   - Asegurar que el dataset est√© correctamente preprocesado (normalizaci√≥n, codificaci√≥n correcta de etiquetas).

- **Si el modelo tarda demasiado en entrenar**:  
   - Aumentar el tama√±o del **batch** (`batch_size`).
   - Usar **Transfer Learning** si es un problema de im√°genes.
   - Aplicar **cuantizaci√≥n** o **pruning** si el modelo es muy grande.

- **Si el modelo muestra signos de sobreajuste**:  
   - Aplicar **Dropout** en capas densas.
   - Usar **Data Augmentation** en im√°genes.
   - Aplicar **L2 Regularization** en los pesos de las capas densas.

##### **Uso de `callbacks` para mejorar el proceso de entrenamiento**
Los **callbacks** permiten mejorar el control sobre el entrenamiento. Los m√°s usados son:

- **Early Stopping** ‚Üí Detiene el entrenamiento si la m√©trica de validaci√≥n deja de mejorar:

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor="val_loss",
    patience=5,  # N√∫mero de √©pocas sin mejora antes de detener
    restore_best_weights=True
)
```

**Reduce Learning Rate on Plateau** ‚Üí Reduce la tasa de aprendizaje si la validaci√≥n deja de mejorar:

```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,  # Reduce LR a la mitad
    patience=3,
    min_lr=1e-6
)
```

**Guardado de pesos del mejor modelo (`ModelCheckpoint`)**  ‚Üí Guarda el modelo con la mejor m√©trica de validaci√≥n:

```python
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint(
    "mejor_modelo.h5",
    monitor="val_loss",
    save_best_only=True,
    verbose=1
)
```

Podemos integrar los callbacks anteriores en un ejemplo de entrenamiento:
```python
modelo.fit(
    X_train, y_train,
    batch_size=32,
    epochs=50,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, reduce_lr, checkpoint]
)
```

##### **Resumen de mejores pr√°cticas en la fase de entrenamiento** 

Puedes considerar la siguiente tabla resumen

| **Problema**                   | **Posible soluci√≥n**                                         |
| ------------------------------ | ------------------------------------------------------------ |
| **P√©rdida no mejora**          | Reducir `learning_rate`, probar `BatchNormalization`         |
| **P√©rdida de validaci√≥n sube** | Aplicar regularizaci√≥n (`L2`, Dropout), usar `EarlyStopping` |
| **Entrenamiento lento**        | Aumentar `batch_size`, usar una GPU o TPU                    |
| **Modelo sobreajustado**       | Aumentar datos (Data Augmentation), a√±adir Dropout           |
| **Gradientes inestables**      | Usar `BatchNormalization`, probar `RMSprop` en vez de Adam   |
| **Dataset desbalanceado**      | Ajustar pesos de clases (`class_weight`) o aumentar datos    |

### **Evaluaci√≥n y m√©tricas en Deep Learning**  

La evaluaci√≥n de modelos en deep learning depende del tipo de tarea: clasificaci√≥n, regresi√≥n o series temporales. La selecci√≥n adecuada de m√©tricas permite entender el rendimiento real del modelo y compararlo con otras alternativas.  

#### **M√©tricas para clasificaci√≥n**  

En problemas de **clasificaci√≥n**, el objetivo es evaluar qu√© tan bien el modelo asigna etiquetas a cada muestra. Existen diferentes m√©tricas seg√∫n el tipo de clasificaci√≥n:  

##### **Precisi√≥n global (`accuracy`)**  
La **exactitud** mide la proporci√≥n de predicciones correctas sobre el total de muestras. Se usa en **problemas balanceados**, pero puede ser enga√±osa en datasets desbalanceados.  

En **Keras**, se usa as√≠:  
```python
from tensorflow.keras.metrics import Accuracy

modelo.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
```

##### **Matriz de confusi√≥n y m√©tricas derivadas**  
La **matriz de confusi√≥n** desglosa los aciertos y errores seg√∫n clases. A partir de ella, se obtienen m√©tricas m√°s detalladas:  

- **Precisi√≥n (`Precision`)**: ¬øCu√°ntos de los positivos predichos eran realmente positivos?  
- **Recall (`Sensibilidad`)**: ¬øCu√°ntos de los positivos reales fueron detectados?  
- **F1-score**: Promedio arm√≥nico entre precisi√≥n y recall, √∫til en datasets desbalanceados.  

En **Python**, se calcula con `sklearn`:  
```python
from sklearn.metrics import classification_report

y_pred = modelo.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)  # Convertir a clases si es one-hot
print(classification_report(y_test, y_pred_classes))
```

##### **Cu√°ndo usar cada m√©trica en clasificaci√≥n**:  

| **Escenario**                                  | **M√©trica recomendada**  |
| ---------------------------------------------- | ------------------------ |
| Dataset balanceado                             | Accuracy                 |
| Dataset desbalanceado                          | F1-score                 |
| Clasificaci√≥n binaria (pocos falsos positivos) | Precision                |
| Clasificaci√≥n binaria (pocos falsos negativos) | Recall                   |
| Multiclase                                     | Matriz de confusi√≥n + F1 |

#### **M√©tricas para regresi√≥n**  

En **regresi√≥n**, el objetivo es minimizar el error entre los valores predichos y los reales. Se utilizan m√©tricas basadas en diferencias entre valores num√©ricos.  

##### **Error cuadr√°tico medio (`MSE`)**  
Mide el error promedio **elevado al cuadrado**, penalizando m√°s los errores grandes. 

Implementaci√≥n en Keras:**

```python
modelo.compile(optimizer="adam", loss="mse", metrics=["mae"])
```

##### **Error absoluto medio (`MAE`)**  
Promedio de las diferencias absolutas entre valores reales y predichos. Es menos sensible a valores extremos que el MSE.

##### **Coeficiente de determinaci√≥n (`R¬≤`)**  
Mide qu√© porcentaje de la variabilidad de los datos es explicado por el modelo. Un valor cercano a **1** indica un ajuste perfecto.  

**Implementaci√≥n en `sklearn`:**  

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred = modelo.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse}, MAE: {mae}, R¬≤: {r2}")
```

**Cu√°ndo usar cada m√©trica en regresi√≥n**:  

| **Escenario**                        | **M√©trica recomendada** |
| ------------------------------------ | ----------------------- |
| Penalizar errores grandes            | MSE                     |
| Penalizar errores peque√±os por igual | MAE                     |
| Comparar con un modelo base          | R¬≤                      |

#### **Evaluaci√≥n en problemas de series temporales**  

Las series temporales tienen caracter√≠sticas espec√≠ficas, por lo que su evaluaci√≥n requiere m√©tricas que tengan en cuenta la dependencia temporal.

##### **Errores est√°ndar (MSE, MAE)**  
Se usan igual que en regresi√≥n, pero considerando el **orden temporal** de los datos.

##### **Error absoluto porcentual medio (`MAPE`)**  
Eval√∫a el error en **t√©rminos relativos**, √∫til cuando las escalas de valores var√≠an.

Ejemplo de c√≥digo en `sklearn`:

```python
import numpy as np

def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"MAPE: {mape}%")
```

##### **Errores en forecasting (s√©ries temporales futuras)**  
Cuando el modelo predice valores futuros, es importante analizar el **comportamiento de los errores a lo largo del tiempo**. Estrategias comunes incluyen:

- **Rolling window evaluation:** Evaluar el error en distintos per√≠odos.  
- **Comparaci√≥n con modelos base:** Evaluar si el modelo supera a un modelo naive (`y(t) = y(t-1)`).  

**Cu√°ndo usar cada m√©trica en series temporales**:  

| **Escenario**                               | **M√©trica recomendada**   |
| ------------------------------------------- | ------------------------- |
| Datos con valores extremos                  | MAE                       |
| Comparaci√≥n de errores en distintas escalas | MAPE                      |
| Predicci√≥n a largo plazo                    | Rolling window evaluation |

#### **Diferencias entre usar m√©tricas dentro o fuera del entrenamiento**

La elecci√≥n de si calcular la m√©trica **dentro del entrenamiento de la red** (mediante `model.compile()`) o **fuera del entrenamiento** (mediante `sklearn` u otros m√©todos) tiene implicaciones importantes en c√≥mo se analiza el rendimiento del modelo.

##### **Uso de m√©tricas dentro del entrenamiento (`model.compile()`)**

Cuando se define una m√©trica dentro de `model.compile()`, Keras la calcula en **cada batch y en cada √©poca** durante el entrenamiento. Esto permite monitorear la evoluci√≥n del modelo en tiempo real.

Las ventajas de este m√©todo son:

- Se obtiene un registro continuo de la m√©trica durante el entrenamiento.
- Permite visualizar curvas de aprendizaje (`loss`, `accuracy`, etc.).
- Se puede usar `EarlyStopping` y `ReduceLROnPlateau` basados en la m√©trica de validaci√≥n.
- Computaci√≥n optimizada en GPU/TPU.

Pero tambi√©n acarrea algunos problemas

- Algunas m√©tricas como `F1-score` o `R¬≤` no est√°n implementadas de forma nativa en `model.compile()`.
- En datasets desbalanceados, `accuracy` puede ser enga√±osa (se necesita un an√°lisis m√°s detallado).
- Solo se refleja la m√©trica en el **dataset de entrenamiento y validaci√≥n**, sin considerar generalizaci√≥n en test.

##### **Uso de m√©tricas fuera del entrenamiento (post-entrenamiento con `sklearn`)**

Calcular m√©tricas despu√©s del entrenamiento permite analizar **en detalle** el rendimiento del modelo, especialmente cuando se requiere una evaluaci√≥n m√°s espec√≠fica en **datos de test o en problemas desbalanceados**. Al igual que el caso anterior este escenario presenta ventajas y desventajas:

Ventajas:

- Se pueden calcular m√©tricas m√°s avanzadas como `F1-score`, `AUC`, `R¬≤`, etc.
- Permite evaluar en un **dataset de test independiente**, reflejando mejor la capacidad de generalizaci√≥n.
- Se pueden construir **matrices de confusi√≥n** y analizar errores espec√≠ficos.
- √ötil para ajustar umbrales de decisi√≥n en modelos con salida `sigmoid`.

Limitaciones:

- No influye en el entrenamiento, por lo que no permite usar `EarlyStopping` o `ReduceLROnPlateau` directamente con estas m√©tricas.
- Requiere calcular predicciones en test manualmente (`y_pred = model.predict(X_test)`).
- Puede ser m√°s costoso computacionalmente si el dataset es grande..

###### **Comparaci√≥n pr√°ctica seg√∫n escenario**

| **Escenario**                      | **M√©trica dentro (`compile`)**                   | **M√©trica fuera (`sklearn`)**        |
| ---------------------------------- | ------------------------------------------------ | ------------------------------------ |
| Monitorizaci√≥n del entrenamiento   | ‚úî Se actualiza en cada √©poca                     | ‚úò No, se calcula despu√©s             |
| Uso de `EarlyStopping`             | ‚úî S√≠, permite detener cuando no mejora           | ‚úò No aplica directamente             |
| C√°lculo de `F1-score` o `R¬≤`       | ‚úò No disponible en `compile`                     | ‚úî Se puede calcular manualmente      |
| An√°lisis en dataset de test        | ‚úò No se eval√∫a directamente                      | ‚úî Se puede evaluar en test           |
| Correcci√≥n de umbrales (`sigmoid`) | ‚úò No permite ajuste manual                       | ‚úî Se puede ajustar seg√∫n necesidades |
| Detecci√≥n de errores espec√≠ficos   | ‚úò Dif√≠cil identificar falsos positivos/negativos | ‚úî Posible con matriz de confusi√≥n    |

Sin duda la mejor estrategia es combinar ambos enfoques:

- Monitorizar `accuracy` y `loss` dentro del entrenamiento para ajustar el modelo en tiempo real.
- Calcular m√©tricas avanzadas en test despu√©s del entrenamiento para evaluar la generalizaci√≥n.
