# Tema 1. Determinación de sistemas de aprendizaje automático (*Machine Learning*). Modelos de machine learning

## Contenidos

> 1. Definición de aprendizaje automático
> 2. Breve recorrido histórico
> 3. **Componentes del ML**
> 4. Etapas en un proyecto ML
> 5. Tipos de ML
>

---

## 3. Componentes de un proyecto de aprendizaje automático

Todo sistema de aprendizaje automático (ML) está compuesto de varios **elementos clave** que, en conjunto, permiten el desarrollo, ajuste y validación del modelo. Cada componente es esencial para lograr un modelo eficiente y preciso. A continuación, vamos a explorar en detalle los elementos principales.

Como se ha dicho, el éxito de un proyecto de aprendizaje automático va a depender de la integración adecuada de cada componente. Los datos deben estar bien representados, los algoritmos correctamente seleccionados y los modelos deben ser evaluados rigurosamente. La capacidad de generalización de un modelo es clave para su éxito en el mundo real, lo que exige un delicado equilibrio entre el ajuste a los datos de entrenamiento y la capacidad de responder bien a datos nuevos.

> [!important]
>
> ##### Datos-Algoritmo-Modelo: Cómo interaccionan
>
> **El algoritmo necesita los datos** para entrenar un modelo. Sin datos, no puede aprender ni ajustarse.
>
> **Los datos pasan por el algoritmo**, que utiliza técnicas matemáticas para identificar patrones ocultos.
>
> **El modelo entrenado** es el resultado del proceso de aprendizaje que el algoritmo ha realizado con los datos.
>
> **Retroalimentación**: En muchos casos, el modelo generado se evalúa y ajusta con nuevos datos, permitiendo mejoras continuas.

### Datos

El **conjunto de datos** es la base sobre la que se construyen los modelos de ML. Estos datos, de diversa naturaleza, pueden ser numéricos, categóricos, imágenes, texto o señales de audio, y deben ser **representativos y de calidad** para evitar sesgos y mejorar la capacidad del modelo para generalizar a nuevos datos.

> **Ejemplo**: En la predicción de **rendimientos agrícolas**, los datos históricos del clima y el uso de fertilizantes se utilizan para entrenar un modelo capaz de predecir la cantidad de producción de cultivos en diferentes condiciones climáticas.

#### Preprocesamiento de los datos

El **preprocesamiento de datos** es una etapa crítica en cualquier proyecto de machine learning, ya que garantiza la calidad y adecuación de los datos antes de que puedan ser utilizados por los algoritmos de aprendizaje. Este proceso incluye varias técnicas fundamentales que ayudan a transformar los datos crudos en un formato adecuado para ser procesado por un modelo.

##### **Limpieza de Datos**

La **limpieza de datos** se refiere a la identificación y corrección o eliminación de **valores perdidos, incorrectos o duplicados**. En muchos conjuntos de datos, es común encontrar valores faltantes o errores que pueden sesgar los resultados o impedir el correcto entrenamiento del modelo. Existen varias estrategias para manejar estos problemas, como **imputar** los valores faltantes (rellenarlos con promedios, medianas o con algún valor predicho) o simplemente eliminarlos si representan una porción pequeña del total de datos.

> **Ejemplo**: En un conjunto de datos médicos, si faltan valores en los registros de edad o sexo de algunos pacientes, es importante decidir si eliminar esos registros o reemplazar los valores faltantes con la media de los datos existentes.

##### **Normalización**

La **normalización** o **escalado de datos** es fundamental cuando los datos contienen variables con diferentes rangos o escalas. Por ejemplo, si una variable mide ingresos anuales en miles de dólares y otra mide edad en años, sus magnitudes pueden ser tan diferentes que afecten el rendimiento de ciertos algoritmos, como la regresión logística o las redes neuronales, que son sensibles a estas diferencias.

Existen diferentes técnicas de normalización, como el **min-max scaling** (ajustar los valores entre 0 y 1) o el **escalado por estandarización** (ajustar los valores para que tengan media 0 y desviación estándar 1).

> **Ejemplo**: En un modelo que predice precios de viviendas, las variables como el tamaño en metros cuadrados y el número de habitaciones deben estar en escalas comparables para evitar que una domine sobre la otra.

##### **Codificación de Categorías**

En muchos casos, los datos contienen **variables categóricas** (por ejemplo, “bajo”, “medio” y “alto” o “rojo”, “verde” y “azul”) que deben ser transformadas en valores numéricos para que los modelos de machine learning puedan procesarlos. Existen varias técnicas para realizar esta conversión, siendo las más comunes la **codificación one-hot** (crear una columna binaria para cada categoría) y la **codificación ordinal** (asignar valores numéricos basados en un orden establecido).

> **Ejemplo**: En un modelo que predice si un cliente comprará un producto, la variable “nivel de ingresos” puede ser categórica (“bajo”, “medio”, “alto”). Para que el modelo la entienda, se podría convertir en 1, 2 y 3 respectivamente.

##### Para Reflexionar...

> **¿Qué problemas pueden surgir si no se realiza una adecuada normalización de los datos?**
> **Clave**: Sin normalización, los modelos sensibles a las magnitudes de las variables, como las redes neuronales, podrían dar más peso a variables con mayores escalas, distorsionando los resultados.

> **¿Cuándo es preferible eliminar datos faltantes en lugar de imputarlos?**
> **Clave**: Considera la proporción de valores faltantes y si eliminarlos afectaría la representatividad del conjunto de datos o introduciría sesgos.

> **¿Qué diferencias existen entre las técnicas de codificación one-hot y ordinal, y cuándo es recomendable usar cada una?**
> **Clave**: Reflexiona sobre cómo las relaciones entre categorías influyen en la elección de la técnica adecuada, ya que la codificación ordinal asume un orden jerárquico, mientras que la one-hot trata cada categoría como independiente.

#### Entrenamiento, test y validación

En un proyecto de *machine learning* (ML), es crucial separar los datos en **conjuntos de entrenamiento, validación y test**. Esta práctica asegura que el modelo se entrena correctamente, se evita el sobreajuste, y se evalúa de manera precisa su rendimiento. Cada conjunto cumple una función específica en el proceso de desarrollo del modelo. A continuación, detallamos estos tres conceptos clave:

##### **Conjunto de entrenamiento**

El **conjunto de entrenamiento** es el que se utiliza para ajustar los parámetros del modelo. Aquí, el modelo de ML aprende los patrones subyacentes en los datos. Durante el entrenamiento, el algoritmo ajusta sus parámetros internos (por ejemplo, los pesos en una red neuronal) con el fin de minimizar un error o maximizar un rendimiento, como la precisión o el rendimiento general en la tarea deseada.

Por ejemplo, en un problema de clasificación de imágenes, el conjunto de entrenamiento incluiría imágenes etiquetadas (como “gato” o “perro”), y el modelo ajustaría sus parámetros para reconocer estas clases.

##### **Conjunto de validación**

El **conjunto de validación** se utiliza durante el entrenamiento para monitorizar el rendimiento del modelo y ajustar hiperparámetros (como la tasa de aprendizaje, el número de capas en una red neuronal, etc.). No se usa para entrenar el modelo directamente, sino para verificar su comportamiento en datos que no ha visto antes, pero que siguen siendo parte del proceso de optimización.

El conjunto de validación es fundamental para **evitar el sobreajuste** (overfitting), un problema común cuando el modelo aprende demasiado bien los detalles del conjunto de entrenamiento, y pierde capacidad de generalización en datos nuevos. Durante el proceso de entrenamiento, si el rendimiento en el conjunto de entrenamiento es alto pero bajo en el de validación, es probable que el modelo esté sobreajustado.

> **Ejemplo**: Durante el desarrollo de un modelo de predicción de ventas, el conjunto de validación se utilizaría para verificar si el modelo predice correctamente en diferentes meses o estaciones, ayudando a ajustar el modelo para mejorar su rendimiento en diferentes contextos temporales.

##### **Conjunto de test**

El **conjunto de test** es la evaluación final del modelo, y se utiliza **una vez completado el entrenamiento**. A diferencia del conjunto de validación, los datos de test son completamente nuevos para el modelo y permiten medir su rendimiento en un entorno simulado que represente datos reales en producción.

Una vez que el modelo ha sido ajustado utilizando el conjunto de validación, se aplica al conjunto de test para obtener una estimación precisa de su rendimiento general. **No se debe ajustar el modelo basándose en los resultados del conjunto de test**, ya que esto también podría generar sobreajuste.

> [!important]
>
> ##### El flujo de los datos en un proyecto ML
>
> 1. **Entrenamiento**: Ajusta los parámetros del modelo.
> 2. **Validación**: Ayuda a ajustar hiperparámetros y monitorizar el sobreajuste.
> 3. **Test**: Proporciona una evaluación final del rendimiento.

##### Para Reflexionar...

> **¿Qué problemas podrían surgir si no se utiliza un conjunto de validación?**
> **Clave**: Sin un conjunto de validación, el modelo podría sobreajustarse a los datos de entrenamiento y tener un mal rendimiento en datos nuevos.

> **¿Cómo asegurar que el conjunto de test refleja adecuadamente el entorno de producción?**
> **Clave**: Se deben elegir cuidadosamente los datos de test para que representen correctamente los casos de uso que el modelo encontrará en producción.

### Modelo

En general, un **modelo** es una representación simplificada de un sistema, proceso o fenómeno del mundo real. Los modelos se utilizan para comprender, predecir o controlar el comportamiento de sistemas complejos, y pueden ser expresados en diferentes formas, como matemáticas, diagramas, simulaciones o estructuras conceptuales.

Un modelo captura los aspectos esenciales de la realidad **mientras omite detalles innecesarios para un propósito específico**. Dependiendo del contexto, los modelos pueden ser descriptivos, prescriptivos o predictivos. Por ejemplo, en física, un modelo puede describir la relación entre variables mediante ecuaciones matemáticas, mientras que en economía, un modelo puede predecir el comportamiento de mercados bajo ciertos supuestos.

**En machine learning**, un modelo es una representación matemática de un sistema que se utiliza para hacer predicciones o tomar decisiones basadas en datos. Intuitivamente, se puede imaginar como una "receta" que se aprende a partir de datos de entrenamiento. A través del aprendizaje de ejemplos previos, el modelo identifica patrones y reglas en los datos que luego le permiten predecir resultados para nuevos datos que no ha visto antes.

Por ejemplo, en un modelo para predecir el precio de una vivienda, el modelo aprende a relacionar características como el tamaño, la ubicación y el número de habitaciones con el precio. Luego, cuando recibe nuevos datos (una casa diferente con características similares), utiliza estas reglas aprendidas para estimar su precio. 

Todo modelo de machine learning toma **entradas** (datos de entrada) y produce **salidas** (predicciones o clasificaciones), con el objetivo de realizar inferencias sobre datos no observados. Este proceso implica encontrar la **función matemática** que mejor describe la relación entre las entradas y salidas, ajustando los parámetros del modelo para minimizar el **error de predicción**.

El **entrenamiento** del modelo consiste en iterar sobre un conjunto de datos etiquetados, ajustando los **parámetros** del modelo mediante un proceso de **optimización**, generalmente minimizando una **función de coste**. Esto asegura que el modelo sea capaz de generalizar, es decir, que no solo funcione bien con los datos de entrenamiento, sino que también sea preciso en los datos nuevos.

#### Complejidad de los modelos de ML

Los modelos de ML pueden clasificarse según su complejidad.

En el contexto del aprendizaje automático, la distinción entre **modelos lineales** y **modelos no lineales** es crucial para entender cómo diferentes técnicas abordan el problema de modelar relaciones entre variables. Representan dos tipos diferentes de enfoques para modelar la relación entre las entradas (características) y las salidas (predicciones). La elección entre estos modelos dependerá evidentemente de la naturaleza del problema y los datos.

##### Modelos lineales

Un **modelo lineal** asume que la relación entre las variables de entrada (o características) y la salida es una combinación lineal de las entradas. Matemáticamente, se expresa como:

$$
y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n
$$

donde:

- $y$ es la salida (la predicción),
- $w_0$ es el **término independiente**
- $w_1, w_2, \dots, w_n$ son los **coeficientes** o pesos asociados a las variables de entrada.
- $x_1, x_2, \dots, x_n$ son las variables de entrada (o características).

El modelo lineal ajusta estos parámetros ($w_0, w_1, \dots, w_n$) durante el entrenamiento, minimizando el error entre las predicciones $y$ y los valores reales. Los modelos lineales son fáciles de interpretar y entrenar, pero su capacidad para modelar relaciones complejas es limitada.

###### Ventajas e inconvenientes de los modelos lineales

**Ventajas**:

- Simplicidad y facilidad de interpretación: se puede ver fácilmente cómo cada entrada influye en la salida.
- Rápidos de entrenar y eficientes en problemas con relaciones sencillas entre las variables.

**Desventajas**:

- No pueden capturar relaciones complejas o no lineales entre las variables.
- Suponen que los efectos combinados de las entradas son aditivos y proporcionales, lo que limita su flexibilidad.

##### Modelos no lineales

En contraste, los **modelos no lineales** permiten que las relaciones entre las entradas y la salida sean mucho más complejas. Los modelos no lineales pueden capturar interacciones y relaciones no triviales que un modelo lineal no puede. Una función no lineal puede tomar muchas formas, como por ejemplo:

$$
y = w_0 + w_1 x_1^2 + w_2 \sin(x_2) + w_3 e^{x_3}
$$

Este tipo de modelos puede incluir términos polinómicos, funciones trigonométricas, exponenciales, o incluso relaciones definidas por redes neuronales. Los modelos no lineales son muy flexibles y pueden ajustar patrones complejos en los datos, pero también tienen mayor riesgo de **sobreajuste** si no se gestionan adecuadamente.

###### Ventajas y Desventajas de los Modelos No Lineales

**Ventajas**:

- Mayor capacidad para capturar relaciones complejas en los datos.
- Flexibilidad para adaptarse a una amplia variedad de patrones, independientemente de la forma que tomen.

**Desventajas**:

- Requieren más datos para entrenarse de manera eficaz, ya que son más propensos al **sobreajuste** (overfitting).
- Más difíciles de interpretar, lo que puede ser un problema en contextos donde se requiere explicabilidad del modelo.
- Típicamente más costosos computacionalmente.



> [!important]
>
> **Modelos lineales**: adecuados cuando las relaciones entre las variables son aproximadamente lineales. Son interpretables, fáciles de entrenar y computacionalmente baratos.
>
> **Modelos no lineales**: se utilizan cuando hay una relación compleja entre las características y la variable objetivo. Tienen mayor capacidad predictiva, pero son más difíciles de interpretar y entrenar, y a menudo requieren más datos para evitar sobreajuste.



> **Ejemplo:** Supongamos que queremos predecir el **precio de una vivienda** en función de una característica simple, como el **tamaño** de la casa ($x$). Un **modelo lineal** podría asumir que el precio ($y$) depende linealmente del tamaño de la vivienda, es decir:
> $$
> y = w_0 + w_1 x
> $$
>
> En este caso, el modelo asume que el precio aumenta proporcionalmente con el tamaño. Esto refleja una **relación lineal** entre el tamaño de la casa ($x$) y el precio ($y$), donde $w_0$ es el término independiente y $w_1$ es la pendiente de la recta.
>
> Sin embargo, el precio de una vivienda no suele depender únicamente de su tamaño. Factores más complejos, como la **ubicación**, el **número de habitaciones** y la **antigüedad** de la vivienda, pueden influir de forma no lineal. En este caso, un **modelo no lineal** sería más adecuado. Por ejemplo, un modelo podría tomar la forma:
>
> $$
> y = w_0 + w_1 x_1 + w_2 x_2^2 + w_3 \log(x_3)
> $$
>
> Aquí, la **ubicación** ($x_1$) tiene una relación lineal con el precio, el **número de habitaciones** ($x_2$) tiene una relación cuadrática, y la **antigüedad** ($x_3$) afecta de manera logarítmica al precio. Este tipo de modelo no lineal permitiría capturar mejor las interacciones complejas entre las distintas características de las viviendas.
>

> **Ejemplo:** Un ejemplo típico de **modelo No Lineal** sería el del uso de **redes neuronales profundas** para el **reconocimiento de imágenes**. En este caso, las relaciones entre los píxeles de una imagen y la etiqueta de la clase (por ejemplo, "gato" o "perro") son altamente no lineales, y el modelo debe aprender patrones complejos en las imágenes.

##### Para reflexionar...

> **¿Cuándo preferirías utilizar un modelo lineal sobre un modelo no lineal, y qué implicaciones tiene esa elección en términos de rendimiento y complejidad?**
>
> **Clave**: Reflexiona sobre cómo la simplicidad del modelo afecta la capacidad de generalización y el riesgo de sobreajuste.

> **¿En qué casos podría un modelo lineal ser insuficiente para capturar la relación entre las variables de entrada y salida?**  
> **Clave**: Reflexiona sobre situaciones en las que las entradas tienen efectos no aditivos o proporcionales en la salida, como en datos con interacciones complejas.

> **¿Cómo afecta la capacidad de generalización al comparar modelos lineales y no lineales?**  
> **Clave**: Considera cómo un modelo lineal tiende a generalizar bien en conjuntos de datos pequeños, mientras que un modelo no lineal podría requerir más datos y ser propenso al sobreajuste.

#### Entrenamiento y generalización

Durante el entrenamiento, el modelo ajusta sus parámetros para **minimizar el error** en las predicciones, utilizando algún algoritmo de optimización como por ejemplo el denominado de ***gradiente descendente***. El objetivo es lograr que el modelo no solo memorice los datos de entrenamiento, sino que sea capaz de **generalizar** a datos nuevos y no vistos previamente. Si el modelo es demasiado simple, puede sufrir de **subajuste** (no capturar adecuadamente los patrones), y si es demasiado complejo, puede **sobreajustar** (memorizar los datos sin generalizar).

Así pues, el ajuste de la complejidad del modelo en **machine learning** es esencial. Dos de las técnicas más comunes para gestionar este equilibrio son la **regularización** y la **validación cruzada**. Aunque ya se ampliaran estos conceptos en capítulos próximos, de momento podemos entender la  **regularización** como una técnica que introduce penalizaciones en el cálculo de los parámetros del modelo para evitar que se ajusten demasiado a los datos de entrenamiento, lo que mejora la capacidad de generalización. Por su parte, la **validación cruzada** es otra técnica que trata de dividir los datos en subconjuntos para entrenar y evaluar el modelo de manera repetida, asegurando que el rendimiento se mida de manera consistente en diferentes partes de los datos.

##### Para reflexionar...

> **¿Cómo influye la complejidad del modelo en su capacidad para generalizar a nuevos datos?**
>
> **Clave**: Reflexiona sobre el equilibrio entre un modelo que es lo suficientemente complejo para capturar patrones importantes, pero no tan complejo que se ajuste demasiado a los datos de entrenamiento.

> **¿Por qué un modelo lineal puede no ser adecuado para problemas con relaciones no lineales?**
>
> **Clave**: Considera ejemplos donde las variables tienen relaciones complejas o interacciones que no se pueden capturar con una simple línea recta.

#### Evaluación del modelo

La **evaluación de modelos** es una etapa crítica en cualquier proyecto de **machine learning**. Su objetivo es medir el rendimiento del modelo y su capacidad para **generalizar** correctamente a partir de datos de entrada **que no ha visto antes**. Si un modelo se ajusta bien a los datos de entrenamiento pero falla al enfrentarse a nuevos datos no es útil en la práctica. Aquí es donde entran en juego las **métricas de evaluación**, que proporcionan **una forma cuantitativa** de medir el rendimiento del modelo.

Una **métrica** es una función que mide algún aspecto del rendimiento del modelo. Según el tipo de problema (clasificación, regresión, etc.), diferentes métricas se utilizan para proporcionar información sobre la calidad de las predicciones.

##### Principales métricas de evaluación

Es importante primeramente introducir el concepto de **matriz de confusión** para entender el resto de métricas. La **matriz de confusión** es una herramienta fundamental en la evaluación de modelos de clasificación. Permite visualizar el rendimiento del modelo al mostrar las predicciones realizadas frente a los valores reales. Se organiza en una matriz de 2x2 (en el caso más simple de problemas binarios), con las siguientes categorías:

|                   | Predicción Positiva       | Predicción Negativa       |
| ----------------- | ------------------------- | ------------------------- |
| **Real Positivo** | Verdaderos Positivos (TP) | Falsos Negativos (FN)     |
| **Real Negativo** | Falsos Positivos (FP)     | Verdaderos Negativos (TN) |

- **True Positive (TP)**: Predicciones correctas de la clase positiva (predicho positivo y es realmente positivo).
- **False Positive (FP)**: Predicciones incorrectas de la clase positiva (predicho positivo pero es realmente negativo).
- **True Negative (TN)**: Predicciones correctas de la clase negativa (predicho negativo y es realmente negativo).
- **False Negative (FN)**: Predicciones incorrectas de la clase negativa (predicho negativo pero es realmente positivo).

A partir de esta matriz, se derivan varias métricas clave:

- **Precisión**: $\frac{TP}{TP + FP}$ mide la proporción de predicciones positivas correctas.
- **Recall (sensibilidad)**: $\frac{TP}{TP + FN}$ mide la capacidad del modelo para identificar correctamente los casos positivos.
- **F1-Score**: La media armónica de precisión y recall, útil en casos desbalanceados.
- **Exactitud**: $\frac{TP + TN}{TP + TN + FP + FN}$ que mide la proporción de predicciones correctas. 

Esta matriz es una herramienta poderosa para evaluar y mejorar el rendimiento de los modelos en problemas de clasificación.

###### Exactitud

Indica el porcentaje de predicciones correctas en relación con el total de predicciones realizadas. Es útil cuando el coste de las predicciones incorrectas es similar para todas las clases, pero puede ser engañosa en problemas con clases desbalanceadas.

$$\text{Precisión} = \frac{TP + TN}{TP + TN + FP + FN}$$

> **Ejemplo:** Un modelo de reconocimiento facial identifica correctamente 98 de 100 rostros, logrando una exactitud del 98%.

###### Precisión

La **precisión** mide el porcentaje de casos **predichos como positivos** que **realmente son positivos**. Es útil cuando nos importa conocer la proporción de verdaderos positivos entre todos los elementos que el modelo ha clasificado como positivos. Su fórmula es:

$$\text{Precisión} = \frac{TP}{TP + FP}$$

> **Ejemplo**: En un clasificador de detección de fraudes, una alta precisión significa que la mayoría de las transacciones etiquetadas como fraudulentas son efectivamente fraudes.

###### **Recall** (o sensibilidad)

Mide la capacidad del modelo para detectar correctamente las verdaderas instancias positivas. Es especialmente útil cuando es crítico identificar todos los positivos, como en la detección de enfermedades.

$$ \text{Recall} = \frac{TP}{TP + FN} $$

> **Ejemplo**: En la detección de cáncer, es vital que el modelo detecte todos los casos de cáncer (recall alto), aunque ocasionalmente marque falsos positivos.

###### **F1-Score**

Es la media armónica entre precisión y recall. Es útil en situaciones con datos desbalanceados, donde es importante equilibrar ambas métricas.

$$ \text{F1} = 2 \times \frac{\text{Precisión} \times \text{Recall}}{\text{Precisión} + \text{Recall}} $$

> **Ejemplo**: En un sistema de clasificación de spam, donde es importante tanto detectar correctamente los correos no deseados como minimizar el número de correos válidos etiquetados como spam, el F1-Score sería una métrica clave.

###### **Área bajo la curva ROC (AUC-ROC)**

Mide el rendimiento de un modelo de clasificación para todas las posibles configuraciones de umbrales de decisión. Es particularmente útil para comparar diferentes modelos de clasificación y obtener una visión más holística de su rendimiento.

> **Ejemplo**: Para un clasificador binario, el AUC-ROC puede ayudar a medir cómo de bien separa el modelo las dos clases bajo distintos umbrales de probabilidad.

##### ¿Cómo elegir la métrica adecuada?

Visto lo anterior surge la cuestión de cómo elegir la **métrica adecuada** en un problema de **machine learning**. Ello va a depender del contexto del problema y del objetivo del modelo. A continuación se detallan algunos factores clave a considerar.

###### Desbalance de clases

Si las clases están desbalanceadas, es decir, una clase tiene muchos más ejemplos que la otra (por ejemplo, en la detección de fraudes, donde los casos fraudulentos son mucho menos frecuentes), métricas como la **exactitud** pueden ser engañosas. En este caso, es mejor utilizar métricas que se centren en los casos positivos, como la **sensibilidad** o la **precisión**.

###### **Tipo de error crítico**

Depende de si los **falsos positivos** o los **falsos negativos** son más críticos para el problema. La **sensibilidad** es crítica cuando los **falsos negativos** tienen un costo elevado, como en el diagnóstico de enfermedades (es preferible detectar todos los casos). Sin embargo la **precisión** es más importante cuando los **falsos positivos** son costosos, como en la detección de fraudes (marcar transacciones legítimas como fraudulentas puede dañar la confianza del cliente).

###### **Equilibrio entre precisión y sensibilidad**

Si ambos tipos de errores son importantes, es útil utilizar una métrica que equilibre precisión y sensibilidad. Para ello se dispone del **F1-Score**, que como hemos comentado es la media armónica entre precisión y sensibilidad y útil cuando se busca un balance entre ambas.

###### **Exactitud en problemas balanceados**

Cuando las clases están bien representadas en los datos y los errores tienen un costo similar, la **exactitud** puede ser una métrica adecuada, ya que proporciona una visión global del rendimiento del modelo.

> **Ejemplos de elección de métrica:**
>
> **Detección de cáncer (problema crítico de salud)**: La **sensibilidad** es clave, ya que es preferible detectar todos los posibles casos de cáncer (minimizar los falsos negativos), aunque se generen más falsos positivos.
>
> **Sistema de recomendaciones**: La **precisión** es más importante, ya que recomendar productos irrelevantes puede reducir la satisfacción del usuario (minimizar los falsos positivos).
>
> **Spam en correos electrónicos**: El **F1-Score** puede ser útil si se busca un balance entre detectar spam (recall) y evitar que correos válidos se marquen como spam (precisión).

##### Para reflexionar...

> **¿Cuándo es más relevante el uso del F1-Score frente a otras métricas como la precisión?**
> **Clave**: Reflexiona sobre la importancia del equilibrio entre precisión y recall, especialmente en problemas con clases desbalanceadas.

> **¿Por qué la AUC-ROC es una métrica relevante en problemas de clasificación?**
> **Clave**: Considera cómo esta métrica permite evaluar el rendimiento de un clasificador en distintos umbrales, proporcionando una visión completa de su desempeño.

> **¿Qué riesgos puede haber al usar solo la precisión como métrica en problemas desbalanceados?**
> **Clave**: Piensa en cómo un alto valor de precisión puede ocultar un rendimiento pobre en la identificación de la clase minoritaria.

##### A debate...

> **¿Debe primar la precisión de un modelo o su capacidad de generalización?**
>
> **Clave**: Reflexiona sobre los casos donde un modelo altamente preciso en los datos de entrenamiento puede fallar con nuevos datos, indicando un problema de **sobreajuste**.

### Algoritmos

Un **algoritmo de aprendizaje automático** es un conjunto formal de procedimientos matemáticos y reglas que permiten a un sistema aprender patrones a partir de los datos, con el fin de hacer predicciones, clasificaciones o tomar decisiones. Su base teórica hay que buscarla en la estadística, el álgebra lineal y la optimización matemática. El objetivo de cualquier algoritmo en aprendizaje automático es minimizar una **función de coste** (o también una **función de pérdida**) que representa el **error** del modelo al realizar sus objetivos marcados. La diferencia fundamental entre función de pérdida y función de coste estriba en que la primera mide el error en una sola predicción, mientras que la segunda se configura a partir de la agregación de los errores (generalmente el promedio) en todas las instancias de los datos de entrenamiento.

#### Un poco de matemáticas...
Los algoritmos de machine learning se fundamentan en principios matemáticos que optimizan las funciones objetivo, ajustando los parámetros del modelo para minimizar errores y maximizar el rendimiento. Como hemos comentado más arriba, la mayoría de estos algoritmos se basan en la **minimización de una función de coste** o **pérdida**.

> Ejemplo: En la regresión lineal, el objetivo es encontrar los coeficientes de una función que minimice la suma de los errores cuadráticos entre las predicciones y las observaciones reales.

El concepto de **derivada** es crucial en machine learning porque permite calcular los cambios en la función objetivo en respuesta a modificaciones en los parámetros del modelo. Esto es esencial para entrenar modelos y minimizar errores en predicciones. La derivada de una función mide la pendiente o el ritmo de cambio de la función en un punto. En machine learning, la función de pérdida (o en su caso de coste), que mide el error entre las predicciones del modelo y los valores reales se optimiza encontrando los parámetros del modelo que minimicen esta función. Las derivadas parciales respecto a cada parámetro indican cómo cambiar cada parámetro para reducir el error.

A partir del concepto de **derivada parcial** podemos obtener el de **gradiente**. El **gradiente** es una de las herramientas más utilizadas en optimización. Es un **vector** que contiene las derivadas parciales de la función objetivo con respecto a cada uno de los parámetros del modelo.

El concepto de gradiente puede aplicarse en el algoritmo denominado **gradiente descendente**. Este algoritmo es uno de los algoritmos de optimización más utilizados en entrenamiento de modelos de machine learning.

> **Ejemplo**: En un modelo de regresión lineal, la derivada de la función de pérdida con respecto a los coeficientes indica si se deben aumentar o disminuir los coeficientes para acercarse a la solución óptima.

Por su parte, las derivadas también están presentes en el campo de las redes neuronales, ya que son fundamentales para el algoritmo de **retropropagación** (*backpropagation*). Este algoritmo calcula cómo los errores se propagan a través de las capas de la red. Usando derivadas, el algoritmo ajusta los pesos en cada capa para mejorar la precisión del modelo.

##### Para reflexionar...
> **¿Cómo afecta el cálculo incorrecto del gradiente a la convergencia de un algoritmo de optimización?**
> **Clave**: Un gradiente mal calculado podría dirigir el modelo en la dirección incorrecta, empeorando el rendimiento.

#### Parámetros vs. datos disponibles

El número de parámetros de un modelo define su **complejidad**. Si el modelo tiene muchos parámetros, puede ajustar mejor los datos de entrenamiento, lo que aumenta su capacidad para capturar patrones. Sin embargo, si el número de datos disponibles no es suficiente en relación con la cantidad de parámetros del modelo, esto puede generar **sobreajuste** (**overfitting**).

El **sobreajuste** ocurre cuando un modelo se ajusta tan bien a los datos de entrenamiento que también aprende el ruido y las variaciones aleatorias en los datos. Esto significa que, aunque su desempeño en el conjunto de entrenamiento es excelente, no generaliza bien en datos nuevos, ya que está "demasiado especializado" en el conjunto de entrenamiento.

Por otro lado, un modelo con **pocos parámetros** puede ser demasiado simple para representar adecuadamente las relaciones presentes en los datos. En este caso, el modelo sufriría de **subajuste** (**underfitting**), lo que significa que no captura correctamente los patrones subyacentes de los datos. Este fenómeno lleva a un mal rendimiento tanto en el entrenamiento como en la validación. 

Una regla empírica comúnmente usada en entrenamiento de modelos sugiere que, para estos generalicen bien, se necesita al menos **10 veces más instancias de datos que parámetros**. Es decir, si un modelo tiene 1,000 parámetros, se recomienda tener al menos 10,000 instancias de datos. Si no se puede alcanzar esta relación óptima, se deberán utilizar técnicas de **regularización**, que reducirán la complejidad del modelo y así evitar el sobreajuste.

> **Ejemplo:** Imagina que entrenas un modelo para predecir el precio de casas. Un modelo simple (como una regresión lineal con un solo parámetro, basado solo en el tamaño de la casa) puede sufrir de **subajuste**, ya que no captura otros factores como la ubicación o el número de habitaciones. Por otro lado, un modelo muy complejo (una red neuronal profunda con muchos parámetros) puede **sobreajustar** los datos de entrenamiento, reconociendo detalles irrelevantes y no generalizando bien a datos de nuevas casas.

#### Optimización y Ajuste
El ajuste de modelos en *machine learning* implica encontrar el conjunto óptimo de parámetros que minimice una función de pérdida o costo, lo que se conoce como **optimización**. Este proceso de optimización busca maximizar el rendimiento del modelo al aprender de los datos de entrenamiento y generalizar a datos nuevos. Existen distintos métodos de optimización utilizados para ajustar modelos, y entre los más comunes podemos encontrar **algoritmo de mínimos cuadrados ordinario** o el **gradiente descendente** entre otros

##### Algoritmo de mínimos cuadrados

El más simple de todos los algoritmos de optimización es sin duda el **algoritmo de mínimos cuadrados ordinarios (OLS)**. El algoritmo de mínimos cuadrados es una técnica utilizada para optimizar modelos en **problemas de regresión**. Su objetivo es encontrar la relación entre una o más variables independientes (también llamadas **características** o *features*) y una variable dependiente (la **salida** o resultado), y que minimice la suma de los errores entre las predicciones del modelo y los valores observados, lo que se conoce como error cuadrático (SSE)

En una **regresión lineal simple**, el modelo que se ajusta es de la forma:

$$
y = \theta_0 + \theta_1 x
$$
Donde:
- $ y $ es el valor que queremos predecir.
- $ x $ es la característica independiente.
- $ \theta_0 $ es el término independiente.
- $ \theta_1 $ es la pendiente o coeficiente de $ x $.

El objetivo de OLS es encontrar los valores óptimos de $ \theta_0 $ y $ \theta_1 $ que minimicen la **suma de los errores cuadráticos** (SSE), donde el error es la diferencia entre los valores observados $ y_i $ y las predicciones $ \hat{y}_i $.

$$
SSE = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
$$
El SSE es la suma de los cuadrados de los errores y se minimiza resolviendo el sistema de ecuaciones lineales que surge de las derivadas parciales con respecto a $ \theta_0 $ y $ \theta_1 $. Para minimizar la suma de errores, calculamos las derivadas parciales de la función de coste respecto a los parámetros $ \theta_0 $ y $ \theta_1 $ y las igualamos a cero. Esto da lugar a las **ecuaciones normales**:

$$
\theta = (X^T X)^{-1} X^T y
$$
Donde:
- $ \theta $ es el vector de parámetros que incluye $ \theta_0 $ y $ \theta_1 $.
- $ X $ es la matriz de las características (con una columna de 1's para el término independiente).
- $ y $ es el vector de los valores observados.
- $ X^T $ es la traspuesta de la matriz $ X $.
- $ (X^T X)^{-1} $ es la inversa de $ X^T X $, siempre que exista.

En términos geométricos, OLS **ajusta la línea que pasa más cerca de todos los puntos en el espacio de características**, reduciendo al mínimo las distancias verticales al cuadrado desde cada punto a la línea.

> **Ejemplo**: Supongamos que estamos estudiando la relación entre el **tamaño de una casa (en metros cuadrados)** y su **precio (en miles de euros)**. Los datos de entrenamiento podrían ser los siguientes:
> $$
> \begin{array}{|c|c|}
> \hline
> \text{Tamaño (m²)} & \text{Precio (€)} \\
> \hline
> 50 & 200 \\
> 60 & 250 \\
> 70 & 300 \\
> \hline
> \end{array}
> $$
> Aplicando el algoritmo de OLS, primero formamos la matriz $ X $ (incluyendo un término de 1 para el término independiente) y el vector $ y $:
>
> $$
> X = \begin{bmatrix}
> 1 & 50 \\
> 1 & 60 \\
> 1 & 70
> \end{bmatrix}
> , \quad y = \begin{bmatrix}
> 200 \\
> 250 \\
> 300
> \end{bmatrix}
> $$
> A continuación, resolvemos las ecuaciones normales para obtener los valores óptimos de $ \theta_0 $ (término independiente) y $ \theta_1 $ (pendiente). Al hacerlo, podemos generar la ecuación de la recta que mejor se ajusta a los datos, permitiendo hacer predicciones sobre los precios de las casas basados en su tamaño.

##### El gradiente descendente

El **gradiente descendente** es uno de los algoritmos de optimización más utilizados en *machine learning*, especialmente en problemas de aprendizaje supervisado. Este algoritmo ajusta los parámetros del modelo iterativamente en la dirección opuesta al gradiente de la función de costo, buscando minimizar dicha función. A cada paso, el algoritmo evalúa la derivada de la función de costo con respecto a los parámetros actuales del modelo. La clave del éxito del gradiente descendente radica en el **tamaño del paso** (también llamado tasa de aprendizaje): un paso pequeño puede hacer que el proceso sea lento, mientras que un paso demasiado grande puede hacer que el algoritmo no converja.

> [!tip]
>
> En este enlace puedes encontrar un vídeo muy interesante que explica este concepto de forma muy intuitiva:
> https://youtu.be/A6FiCDoz8_4?si=YyE-SEakC4Yqj4xj

